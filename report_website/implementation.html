<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Implementation - Companio</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/styles.css">
    <!-- Highlight.js -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>

    <style>
        code {
          color: #004080;
          padding: 2px 6px;
          border-radius: 4px;
          font-family: Consolas, monospace;
          font-size: 0.95em;
        }
      </style>
      
</head>

<body>


    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
            <a class="navbar-brand" href="index.html">Companio</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="index.html" id="homeDropdown" role="button"
                            data-bs-toggle="dropdown">Home</a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="index.html#abstract">Abstract</a></li>
                        </ul>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="requirementsDropdown" role="button"
                            data-bs-toggle="dropdown">Requirements</a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="requirements.html#project-background">Project
                                    Background</a></li>
                            <li><a class="dropdown-item" href="requirements.html#client-requirements">Client
                                    Requirements</a></li>
                            <li><a class="dropdown-item" href="requirements.html#project-goal">Project Goal</a></li>
                            <li><a class="dropdown-item" href="requirements.html#user-interviews">User Interviews</a>
                            </li>
                            <li><a class="dropdown-item" href="requirements.html#personas">Personas</a></li>
                            <li><a class="dropdown-item" href="requirements.html#moscow">MoSCoW List</a></li>
                        </ul>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="research.html" id="researchDropdown" role="button"
                            data-bs-toggle="dropdown">Research</a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="research.html#project-research">Project Research</a></li>
                            <li><a class="dropdown-item" href="research.html#technology-research">Technology
                                    Research</a></li>

                        </ul>
                    </li>

                    <li class="nav-item"><a class="nav-link" href="hci.html">HCI</a></li>
                    <li class="nav-item"><a class="nav-link" href="design.html">Design</a></li>
                    <li class="nav-item"><a class="nav-link" href="implementation.html">Implementation</a></li>
                    <li class="nav-item"><a class="nav-link" href="testing.html">Testing</a></li>
                    <li class="nav-item"><a class="nav-link" href="evaluation.html">Evaluation</a></li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="appendicesDropdown" role="button"
                            data-bs-toggle="dropdown">Appendices</a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="appendices.html#monthly-videos">Monthly Videos</a></li>
                            <li><a class="dropdown-item" href="appendices.html#user-manual">User Manual</a></li>
                            <li><a class="dropdown-item" href="appendices.html#development-manual">Development
                                    Manual</a></li>
                            <li><a class="dropdown-item" href="appendices.html#privacy">Privacy of Data</a></li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>
    </nav>


    <!-- Page Content -->
    <div class="container mt-5 pt-5">
        <h1 class="text-center">Implementation</h1>

        <!-- backend -->
        <section id="backend" class="section">
            <h2>Backend Implementation</h2>
        
            <h4>Chatbot Interaction (Semantic Kernel)</h4>
            <ul>
            <li>Python</li>
            <li>Semantic Kernel</li>
            <li>FastAPI</li>
            </ul>
        
            <p>This feature is implemented in Python, with the backend structured around FastAPI for handling HTTP requests, and Semantic Kernel as the orchestration engine for LLMs and memory integration. Semantic Kernel is a powerful framework developed by Microsoft to manage prompts, AI services, memory, and function calling in a modular and extensible way. The entire chatbot logic runs asynchronously and supports both online (cloud) and offline (local) models. Communication with the front end is handled via RESTful endpoints built with FastAPI, ensuring smooth request and response streaming.</p>
        
            <h5>AI Service Initialization – initialize_ai_service(kernel)</h5>
            <p>The <code>initialize_ai_service(kernel)</code> method is responsible for registering the appropriate AI services and embedding models into the Semantic Kernel based on the user's selected configuration (online or offline mode).</p>
        
            <p>If offline mode is selected (<code>config.USE_OLLAMA = True</code>), the following snippet initializes a local model using the <code>OpenAIChatCompletion</code> wrapper:</p>
        
            <pre><code class="language-python">openAIClient = AsyncOpenAI(api_key="key", base_url=OLLAMA_BASE_URL)
        kernel.add_service(OpenAIChatCompletion(service_id="local-gpt",
        ai_model_id=config.OLLAMA_MODEL_ID, async_client=openAIClient))</code></pre>
        
            <p>Alongside the LLM, it also registers:</p>
            <ul>
            <li>A dense embedding model: <code>GraniteEmbeddingService</code></li>
            <li>A sparse embedding model: <code>SparseTextEmbedding</code></li>
            <li>A late interaction embedding model: <code>LateInteractionTextEmbedding</code></li>
            <li>A local vectorizer: <code>VectorStoreRecordUtils</code></li>
            <li>The Qdrant vector database client: <code>AsyncQdrantClient</code></li>
            </ul>
        
            <p>If online mode is selected, we instead configure Azure OpenAI models using:</p>
        
            <pre><code class="language-python">kernel.add_service(
            AzureChatCompletion(
            service_id="azure",
            api_key=AZURE_API_KEY,
            deployment_name=AZURE_DEPLOYMENT_NAME,
            endpoint=AZURE_ENDPOINT,
            )
        )</code></pre>
        
            <p>The following additional services are added:</p>
            <ul>
            <li>Embedding service: <code>AzureTextEmbedding</code></li>
            <li>Vectorizer: <code>VectorStoreRecordUtils</code></li>
            <li>Azure AI Search Collection: <code>AzureAISearchCollection[ElderlyUserMemory]</code></li>
            </ul>
        
            <p>These components prepare the kernel to either embed and store user inputs locally or retrieve them from Azure AI Search when online.</p>
        
            <h5>Kernel Setup – setup_kernel()</h5>
            <p>The <code>setup_kernel()</code> method initializes a new <code>Kernel()</code> instance and calls <code>initialize_ai_service(kernel)</code> to configure services appropriately. It then creates a chatbot function using <code>add_function()</code> which embeds system instructions, truncation logic, and past memory directly into the prompt structure.</p>
        
        <pre><code class="language-python">kernel = Kernel()
        service_id, model_name, kernel = await initialize_ai_service(kernel)
        
        chat_function = kernel.add_function(
            plugin_name="ChatBot",
            function_name="Chat",
            prompt=""" ... {{ $user_input }} ... {{ $past_memory }} ... """,
            prompt_execution_settings=settings,
        )</code></pre>
        
            <p>This allows us to easily invoke the chatbot dynamically with contextual awareness, defined in the template using Semantic Kernel’s custom prompt syntax.</p>
        
            <h5>Lazy Initialization – initialize_chatbot()</h5>
            <p>The chatbot system is lazily initialized using the <code>initialize_chatbot()</code> function. This ensures that the <code>kernel</code>, <code>model</code>, and <code>chat_function</code> are only instantiated once, optimizing performance:</p>
        
        <pre><code class="language-python">if kernel is None:
            kernel, chat_function, model_name = await setup_kernel()</code></pre>
        
            <h5>Message Handling – chat() and process_message(user_input)</h5>
            <p>When the user sends input <em>via frontend</em>, the function <code>chat()</code> is triggered. Input is preprocessed and passed into the kernel using <code>invoke_stream()</code>:</p>
        
        <pre><code class="language-python">async for chunk in kernel.invoke_stream(
            chat_function,
            KernelArguments(
            truncation_reducer=truncation_reducer,
            user_input=user_input,
            past_memory=past_memory,
            system_message=config.SYSTEM_MESSAGE
            )
        ): 
            ...</code></pre>
        
            <p>This invocation streams partial results (for low-latency response) and progressively builds the final reply. The output is printed or spoken depending on configuration.</p>
        
            <h5>Memory Handling (Overview)</h5>
            <p>User inputs are categorized and stored as long-term memory if applicable. This is handled using conditional checks:</p>
        
        <pre><code class="language-python">if "collection" in kernel.services:
            await store_memory(kernel, user_id=config.USER_ID, memory_text=user_input,
            category=category)
        elif "qdrant_client" in kernel.services:
            await store_memory_local(kernel, user_id=config.USER_ID, memory_text=user_input,
            category=category)</code></pre>
        
            <p>The actual mechanics of memory storage and retrieval (using Azure AI Search or <code>Qdrant</code>) are discussed in detail in the RAG Memory System section of this report.</p>
        </section>
  
        <section id="rag" class="section">
            <h4>Retrieval-Augmented Generation (RAG) Memory System</h4>
            <p>This feature implements a long-term memory system for conversational AI using both online and offline approaches. The system is built in Python and leverages key tools and frameworks:</p>
            <ul>
              <li><code>Semantic Kernel</code>: Manages embedding, retrieval, and service coordination.</li>
              <li><code>Qdrant</code>: A local vector database used for storing and retrieving memory in offline mode.</li>
              <li><code>Azure AI Search</code>: A cloud-based vector + keyword search system for online RAG.</li>
              <li><code>Pydantic</code>: Used for the ElderlyUserMemory model to define the schema for stored memories.</li>
              <li><code>GraniteEmbeddingService</code>: A custom embedding class using IBM’s Granite model.</li>
              <li><code>FastEmbed</code>: Provides sparse and late interaction embeddings in offline mode.</li>
              <li><code>Torch & Transformers</code>: Used by the GraniteEmbeddingService to generate local embeddings.</li>
            </ul>

            <h5>Online RAG: Azure AI Search Integration</h5>
            <p><strong>Search</strong><br>
            When running in online mode, the user’s input message (retrieved via a POST request) is passed to the <code>search_memory()</code> function along with the active SemanticKernel instance. The kernel internally stores a collection service (an instance of AzureAISearchCollection) and a vectorizer, which generates embeddings using <code>AzureTextEmbedding</code>.</p>

            <pre><code class="language-python">collection = kernel.services["collection"]
            vectorizer = kernel.services["vectorizer"]</code></pre>

            <p>The search flow begins by performing text-based search:</p>

            <pre><code class="language-python">text_results = await collection.text_search(search_text=query, top_k=top_k)</code></pre>

            <p>Simultaneously, vector-based search is performed by generating a query embedding:</p>

            <pre><code class="language-python">query_vector = (await vectorizer.kernel.get_service("embedding").generate_raw_embeddings([query]))[0]
            vector_results = await collection.vectorized_search(vector=query_vector, options=...)</code></pre>

            <p>The results of both searches are then merged using a Reciprocal Rank Fusion (RRF) algorithm in <code>apply_rrf()</code> to prioritize documents that appear highly ranked in both search types. These final memory snippets are returned to the chatbot to be embedded into the prompt for contextual response generation.</p>

            <h5>Store</h5>
            <p>If the chatbot determines that the user input should be remembered (i.e., it’s not a one-off question), it passes the input and metadata (timestamp, category, user_id) to the <code>store_memory()</code> function. This function constructs an <code>ElderlyUserMemory</code> object:</p>

            <pre><code class="language-python">record = ElderlyUserMemory(
            id=f"{user_id}_{timestamp}_{uuid}",
            memory_text=memory_text,
            category=category,
            timestamp=datetime.utcnow().isoformat() + "Z"
            )
            record = await vectorizer.add_vector_to_records(record, ElderlyUserMemory)
            await collection.upsert(record)</code></pre>

            <p>The memory is embedded and upserted into the appropriate Azure AI Search index, associated with the specific user ID.</p>

            <h5>Offline RAG: Local Vector Search with Qdrant</h5>
            <p>In offline mode, the RAG system replaces Azure components with <code>Qdrant</code>, a high-performance vector database hosted locally. During FastAPI server startup, the <code>start_server()</code> method is triggered through the lifespan context manager. This ensures that Qdrant is automatically and reliably started with the application, even when bundled into an executable.</p>

            <pre><code class="language-python">@asynccontextmanager
            async def lifespan(app):
                if not _qdrant_manager.start_server():
                    print("Warning: Failed to start Qdrant server.")
                await initialize_chatbot()
                yield
                _qdrant_manager.stop_server()</code></pre>

            <p>When a message is received, it is passed to <code>search_memory_local()</code>, which interacts with local models through the Semantic Kernel’s <code>kernel.services</code> registry.</p>

            <p>The message is embedded into three different vector types for hybrid search:</p>
            <ul>
            <li>Dense (via <code>GraniteEmbeddingService</code>)</li>
            <li>Sparse (via <code>FastEmbed</code> using BM25)</li>
            <li>Late-Interaction (via <code>FastEmbed</code>)</li>
            </ul>

            <pre><code class="language-python">dense_vectors = await dense_model.generate_raw_embeddings([query])
            sparse_vectors = bm25_model.query_embed(query)
            late_vectors = late_model.query_embed(query)</code></pre>

            <p>These vectors are then sent as a hybrid query to <code>Qdrant</code>:</p>

            <pre><code class="language-python">vector_results = await qdrant_client.query_points(
            QDRANT_COLLECTION,
            prefetch=[...],  # For dense and sparse
            query=late_vectors,
            using="late_interaction_embedding",
            with_payload=True,
            limit=5,
            )</code></pre>

            <p>The retrieved results (stored in <code>result.payload["memory_text"]</code>) are passed back to the chatbot for prompt composition. This ensures the model can generate replies informed by relevant past interactions, even offline.</p>

            <p>If the message should be remembered, it’s passed to <code>store_memory_local()</code> where embeddings are computed and wrapped into a <code>PointStruct</code>. These are inserted into the <code>Qdrant</code> collection using:</p>

            <pre><code class="language-python">await qdrant_client.upsert(collection_name=QDRANT_COLLECTION, points=[record])</code></pre>

            <p>Each memory still receives a UUID and timestamp for tracking.</p>

            <h5>Unified Schema via Pydantic</h5>
            <p>Both systems share a common schema based on the <code>ElderlyUserMemory</code> <code>Pydantic</code> model. This defines how memory records are structured, embedded, and stored:</p>

            <pre><code class="language-python">@vectorstoremodel
            class ElderlyUserMemory(BaseModel):
            id: Annotated[str, VectorStoreRecordKeyField]
            memory_text: Annotated[str, VectorStoreRecordDataField(...)]
            memory_vector: Annotated[list[float], VectorStoreRecordVectorField(...)]
            category: Annotated[str, VectorStoreRecordDataField()]
            timestamp: Annotated[str, VectorStoreRecordDataField()]
            )</code></pre>

            <p>For <code>Qdrant</code>, the schema is manually matched by creating a <code>PointStruct</code> with equivalent keys and vector fields. For Azure, the Semantic Kernel internally parses and embeds the <code>Pydantic</code> model using annotations to create the appropriate index structure.</p>
        </section>

        <section id="speech" class="section">
            <h4>Speech functionalities (TTS/STT)</h4>
            <h5>Speech-to-Text Functionality (STT)</h5>
            <p>To improve accessibility and allow more natural interaction, our system supports voice input through speech-to-text conversion. This feature is available in both online and offline modes, and in both cases, it uses Azure’s Speech Services for its robustness and accuracy.</p>
          
            <p><strong>Technologies Used:</strong></p>
            <ul>
              <li><code>Azure Speech Service</code>: Converts live audio into text using cloud-based recognition.</li>
              <li><code>Microsoft Cognitive Services Speech SDK</code> (in JS): Allows the frontend to directly access Azure STT.</li>
              <li><code>React Hooks</code> (<code>useState</code>, <code>useCallback</code>, <code>useEffect</code>, <code>useRef</code>): Manage recognizer state and UI interactivity.</li>
              <li><code>FastAPI Endpoint</code> <code>/api/azure-credentials</code>: Provides secure access to Azure subscription key and region.</li>
            </ul>
          
            <h5>How It Works (Frontend Flow)</h5>
            <p>When the user clicks the microphone button, the React frontend calls the <code>startRecording</code> method from the VoiceContext:</p>
          
            <pre><code class="language-js">const { startRecording } = useVoice();</code></pre>
          
            <p>This invokes the <code>initializeAzureRecognizer()</code> function. Inside, the app:</p>
            <ol>
              <li>Fetches secure credentials from the backend (<code>/api/azure-credentials</code>).</li>
              <li>Initializes the Azure Speech Recognizer using the Speech SDK:</li>
            </ol>
          
          <pre><code class="language-js">const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(subscriptionKey, region);
          const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
          const recognizer = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);</code></pre>
          
            <li>Registers event listeners for:
              <ul>
                <li><code>.recognizing</code>: for interim transcription</li>
                <li><code>.recognized</code>: for finalized text</li>
                <li><code>.canceled</code>: for errors or interruptions</li>
                <li><code>.sessionStarted</code> / <code>.sessionStopped</code>: to control UI states</li>
              </ul>
            </li>
          
            <li>Starts continuous speech recognition:</li>
            <pre><code class="language-js">recognizer.startContinuousRecognitionAsync(...)</code></pre>
          
            <p>As the user speaks, recognized speech is passed into the state via:</p>
            <pre><code class="language-js">setRecognizedText(event.result.text);</code></pre>
            <p>The final <code>recognizedText</code> is then shown in the chat input.</p>
          
            <h5>Voice Control and Graceful Shutdown</h5>
            <p>The recognizer can be stopped manually with <code>stopRecording()</code> or automatically when voice commands like "stop listening" or "exit program" are detected. If a recording session ends, the recognizer is either closed or reused for future sessions, improving performance and avoiding memory leaks.</p>
          
            <p>Recognizer cleanup logic is also included on component unmount:</p>
            <pre><code class="language-js">useEffect(() => {
            return () => {
              stopCurrentAudio();
              recognizer?.stopContinuousRecognitionAsync();
              recognizer?.close();
            };
          }, []);</code></pre>
          
            <p>Users can speak naturally into their device’s microphone, and the system will transcribe it in real time using Azure STT. Recognized text is reflected in the chat UI and passed to the backend chatbot logic for processing. Voice commands like “stop listening” are interpreted in context and help control the session naturally. This makes the feature both technically robust and friendly for elderly users, as intended.</p>
          
            <h5>Text-to-Speech (TTS) Functionality</h5>
            <p>To make the chatbot more friendly and accessible, especially for elderly users, we implemented Text-to-Speech (TTS). This allows the chatbot to speak its responses out loud using natural-sounding voices. We support both online (cloud-based) and offline TTS, depending on the user's selected configuration.</p>
          
            <p><strong>Technologies Used</strong></p>
            <ul>
              <li><code>Azure Speech Services</code> (Online): Used for high-quality neural voice synthesis in English, with support for both male and female voices.</li>
              <li><code>Coqui TTS</code> (Offline): Lightweight, open-source TTS model that runs locally using PyTorch and torchaudio.</li>
              <li><code>FastAPI</code>: Used to expose the <code>/api/text-to-speech</code> endpoint for serving audio responses as binary WAV files.</li>
              <li><code>React</code> (Frontend): Sends POST requests to the backend and plays back the received audio using a native HTML5 <code>&lt;audio&gt;</code> element.</li>
            </ul>
          
            <h5>How It Works (End-to-End Flow)</h5>
            <p>When a chatbot response is generated, the frontend calls this endpoint:</p>
            <pre><code class="language-js">await fetch('/api/text-to-speech', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ message: responseText }),
          });</code></pre>
          
            <h6>Backend: /api/text-to-speech (FastAPI)</h6>
            <p>This triggers the following logic in the backend:</p>
          
            <pre><code class="language-python">@app.post("/api/text-to-speech")
          async def convert_text_to_speech(input_data: UserInput):
            if config.USE_OLLAMA:
              audio_bytes = speak_text_to_bytes(input_data.message, gender=gender)
            else:
              audio_bytes = text_to_speech_to_bytes(input_data.message, gender=gender)
            return Response(content=audio_bytes, media_type="audio/wav")</code></pre>
          
            <p>If the user is in online mode, the function <code>text_to_speech_to_bytes</code> is called, which uses Azure TTS. If the user is in offline mode, it instead calls <code>speak_text_to_bytes</code>, which uses Coqui TTS.</p>
          
            <h6>Online Mode (Azure TTS)</h6>
            <p>The <code>text_to_speech_to_bytes</code> function performs the following:</p>
          
            <pre><code class="language-python">speech_config = speechsdk.SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SPEECH_REGION)
          speech_config.speech_synthesis_voice_name = "en-US-GuyNeural"  # or Jenny for female
          synthesizer = speechsdk.SpeechSynthesizer(speech_config, audio_config)
          result = synthesizer.speak_text_async(text).get()</code></pre>
          
            <p>The audio is first synthesized and saved to a temporary WAV file. If synthesis is successful, the file is read back as bytes and returned to the user. The file is then safely deleted using a finally block to clean up resources. This ensures high-quality, expressive speech using cloud-powered voices.</p>
          
            <h6>Offline Mode (Coqui TTS)</h6>
            <p>The <code>speak_text_to_bytes</code> function handles the local model:</p>
          
            <pre><code class="language-python">audio_data = tts_model.tts(text=text, speaker=speaker)
          audio_tensor = torch.tensor(audio_data).unsqueeze(0)
          torchaudio.save(buffer, audio_tensor, sample_rate, format="wav")</code></pre>
          
            <ol>
              <li>Text is passed into Coqui’s pre-trained TTS model.</li>
              <li>The audio is returned as a NumPy array, converted into a PyTorch tensor, and saved as a WAV byte stream.</li>
              <li>The final output is streamed to the frontend as playable audio.</li>
            </ol>
          
            <p>Although offline voices are less expressive, they are fast and private, making them ideal for users without internet access.</p>
          
            <h5>Frontend Playback</h5>
            <p>On receiving the audio (offline/online), the React frontend creates an Audio object and plays it like so:</p>
          
            <pre><code class="language-js">const audio = new Audio(URL.createObjectURL(audioBlob));
          audio.play();</code></pre>
          
            <p>We also handle edge cases like overlapping audio, small audio sizes, or muting the output entirely if needed.</p>
          
            <p>With support for both cloud-based and offline text-to-speech, this feature provides a smooth, engaging experience for users. The flexibility in choosing between natural-sounding Azure voices or fully local Coqui synthesis makes the application suitable for both accessibility-focused use cases and resource-constrained environments.</p>
          </section>
          
        <!-- frontend -->
        <section id="frontend" class="section">
            <h2>Frontend Implementation</h2>
            <p>How the frontend was developed...</p>
        </section>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="js/scripts.js"></script>
</body>

</html>