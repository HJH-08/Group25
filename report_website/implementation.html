<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Implementation - Companio</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/styles.css">
    <!-- Highlight.js -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>

    <style>
        code {
          color: #004080;
          padding: 2px 6px;
          border-radius: 4px;
          font-family: Consolas, monospace;
          font-size: 0.95em;
        }
      </style>
      
</head>

<body>


    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
            <a class="navbar-brand" href="index.html">Companio</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="index.html" id="homeDropdown" role="button"
                            data-bs-toggle="dropdown">Home</a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="index.html#abstract">Abstract</a></li>
                        </ul>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="requirementsDropdown" role="button"
                            data-bs-toggle="dropdown">Requirements</a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="requirements.html#project-background">Project
                                    Background</a></li>
                            <li><a class="dropdown-item" href="requirements.html#client-requirements">Client
                                    Requirements</a></li>
                            <li><a class="dropdown-item" href="requirements.html#project-goal">Project Goal</a></li>
                            <li><a class="dropdown-item" href="requirements.html#user-interviews">User Interviews</a>
                            </li>
                            <li><a class="dropdown-item" href="requirements.html#personas">Personas</a></li>
                            <li><a class="dropdown-item" href="requirements.html#moscow">MoSCoW List</a></li>
                        </ul>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="research.html" id="researchDropdown" role="button"
                            data-bs-toggle="dropdown">Research</a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="research.html#project-research">Project Research</a></li>
                            <li><a class="dropdown-item" href="research.html#technology-research">Technology
                                    Research</a></li>

                        </ul>
                    </li>

                    <li class="nav-item"><a class="nav-link" href="hci.html">HCI</a></li>
                    <li class="nav-item"><a class="nav-link" href="design.html">Design</a></li>
                    <li class="nav-item"><a class="nav-link" href="implementation.html">Implementation</a></li>
                    <li class="nav-item"><a class="nav-link" href="testing.html">Testing</a></li>
                    <li class="nav-item"><a class="nav-link" href="evaluation.html">Evaluation</a></li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="appendicesDropdown" role="button"
                            data-bs-toggle="dropdown">Appendices</a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="appendices.html#monthly-videos">Monthly Videos</a></li>
                            <li><a class="dropdown-item" href="appendices.html#user-manual">User Manual</a></li>
                            <li><a class="dropdown-item" href="appendices.html#development-manual">Development
                                    Manual</a></li>
                            <li><a class="dropdown-item" href="appendices.html#privacy">Privacy of Data</a></li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>
    </nav>


    <!-- Page Content -->
    <div class="container mt-5 pt-5">
        <h1 class="text-center">Implementation</h1>

        <!-- backend -->
        <section id="backend" class="section">
            <h2>Backend Implementation</h2>
        
            <h4>Chatbot Interaction (Semantic Kernel)</h4>
            <ul>
            <li>Python</li>
            <li>Semantic Kernel</li>
            <li>FastAPI</li>
            </ul>
        
            <p>This feature is implemented in Python, with the backend structured around FastAPI for handling HTTP requests, and Semantic Kernel as the orchestration engine for LLMs and memory integration. Semantic Kernel is a powerful framework developed by Microsoft to manage prompts, AI services, memory, and function calling in a modular and extensible way. The entire chatbot logic runs asynchronously and supports both online (cloud) and offline (local) models. Communication with the front end is handled via RESTful endpoints built with FastAPI, ensuring smooth request and response streaming.</p>
        
            <h5>AI Service Initialization – initialize_ai_service(kernel)</h5>
            <p>The <code>initialize_ai_service(kernel)</code> method is responsible for registering the appropriate AI services and embedding models into the Semantic Kernel based on the user's selected configuration (online or offline mode).</p>
        
            <p>If offline mode is selected (<code>config.USE_OLLAMA = True</code>), the following snippet initializes a local model using the <code>OpenAIChatCompletion</code> wrapper:</p>
        
            <pre><code class="language-python">openAIClient = AsyncOpenAI(api_key="key", base_url=OLLAMA_BASE_URL)
        kernel.add_service(OpenAIChatCompletion(service_id="local-gpt",
        ai_model_id=config.OLLAMA_MODEL_ID, async_client=openAIClient))</code></pre>
        
            <p>Alongside the LLM, it also registers:</p>
            <ul>
            <li>A dense embedding model: <code>GraniteEmbeddingService</code></li>
            <li>A sparse embedding model: <code>SparseTextEmbedding</code></li>
            <li>A late interaction embedding model: <code>LateInteractionTextEmbedding</code></li>
            <li>A local vectorizer: <code>VectorStoreRecordUtils</code></li>
            <li>The Qdrant vector database client: <code>AsyncQdrantClient</code></li>
            </ul>
        
            <p>If online mode is selected, we instead configure Azure OpenAI models using:</p>
        
            <pre><code class="language-python">kernel.add_service(
            AzureChatCompletion(
            service_id="azure",
            api_key=AZURE_API_KEY,
            deployment_name=AZURE_DEPLOYMENT_NAME,
            endpoint=AZURE_ENDPOINT,
            )
        )</code></pre>
        
            <p>The following additional services are added:</p>
            <ul>
            <li>Embedding service: <code>AzureTextEmbedding</code></li>
            <li>Vectorizer: <code>VectorStoreRecordUtils</code></li>
            <li>Azure AI Search Collection: <code>AzureAISearchCollection[ElderlyUserMemory]</code></li>
            </ul>
        
            <p>These components prepare the kernel to either embed and store user inputs locally or retrieve them from Azure AI Search when online.</p>
        
            <h5>Kernel Setup – setup_kernel()</h5>
            <p>The <code>setup_kernel()</code> method initializes a new <code>Kernel()</code> instance and calls <code>initialize_ai_service(kernel)</code> to configure services appropriately. It then creates a chatbot function using <code>add_function()</code> which embeds system instructions, truncation logic, and past memory directly into the prompt structure.</p>
        
        <pre><code class="language-python">kernel = Kernel()
        service_id, model_name, kernel = await initialize_ai_service(kernel)
        
        chat_function = kernel.add_function(
            plugin_name="ChatBot",
            function_name="Chat",
            prompt=""" ... {{ $user_input }} ... {{ $past_memory }} ... """,
            prompt_execution_settings=settings,
        )</code></pre>
        
            <p>This allows us to easily invoke the chatbot dynamically with contextual awareness, defined in the template using Semantic Kernel’s custom prompt syntax.</p>
        
            <h5>Lazy Initialization – initialize_chatbot()</h5>
            <p>The chatbot system is lazily initialized using the <code>initialize_chatbot()</code> function. This ensures that the <code>kernel</code>, <code>model</code>, and <code>chat_function</code> are only instantiated once, optimizing performance:</p>
        
        <pre><code class="language-python">if kernel is None:
            kernel, chat_function, model_name = await setup_kernel()</code></pre>
        
            <h5>Message Handling – chat() and process_message(user_input)</h5>
            <p>When the user sends input <em>via frontend</em>, the function <code>chat()</code> is triggered. Input is preprocessed and passed into the kernel using <code>invoke_stream()</code>:</p>
        
        <pre><code class="language-python">async for chunk in kernel.invoke_stream(
            chat_function,
            KernelArguments(
            truncation_reducer=truncation_reducer,
            user_input=user_input,
            past_memory=past_memory,
            system_message=config.SYSTEM_MESSAGE
            )
        ): 
            ...</code></pre>
        
            <p>This invocation streams partial results (for low-latency response) and progressively builds the final reply. The output is printed or spoken depending on configuration.</p>
        
            <h5>Memory Handling (Overview)</h5>
            <p>User inputs are categorized and stored as long-term memory if applicable. This is handled using conditional checks:</p>
        
        <pre><code class="language-python">if "collection" in kernel.services:
            await store_memory(kernel, user_id=config.USER_ID, memory_text=user_input,
            category=category)
        elif "qdrant_client" in kernel.services:
            await store_memory_local(kernel, user_id=config.USER_ID, memory_text=user_input,
            category=category)</code></pre>
        
            <p>The actual mechanics of memory storage and retrieval (using Azure AI Search or <code>Qdrant</code>) are discussed in detail in the RAG Memory System section of this report.</p>
        </section>
        <section id="model-switching" class="section">
          <h4>Dynamic Model Switching</h4>
          <p>A key deliverable in our project is dynamic switching between the online and offline models. This allows users to efficiently switch between the two modes within our application. This is achieved through the <code>/api/config</code> FastAPI endpoint which is called when the frontend sends new chatbot settings, such as model type (online or offline), speech output preference, or avatar type. It processes these settings and updates the backend configuration accordingly.</p>

          <p>Initially it stores the the current configuration values:</p>

          <pre><code class="language-python">old_config = config.USE_OLLAMA
        old_model = config.OLLAMA_MODEL_ID if config.USE_OLLAMA else config.AZURE_DEPLOYMENT_NAME
        old_speech = config.USE_SPEECH_OUTPUT</code></pre>

          <p>Then it determines if reinitialization is needed based on configuration changes:</p>

          <pre><code class="language-python">needs_reinitialization = (
          old_config != model_config.use_ollama or 
          old_model != (model_config.model_id if model_config.use_ollama else config.AZURE_DEPLOYMENT_NAME) or
          old_speech != model_config.use_speech
        )

        if needs_reinitialization:
          print(f"Configuration changed, reinitializing chatbot: {model_name} ({mode})")
          background_tasks.add_task(initialize_chatbot)
          return {
            "status": "success",
            "message": f"Configuration updated: {model_name} ({mode} mode), speech {speech_status}{avatar_message}"
          }</code></pre>
        </section>
        <section id="rag" class="section">
            <h4>Retrieval-Augmented Generation (RAG) Memory System</h4>
            <p>This feature implements a long-term memory system for conversational AI using both online and offline approaches. The system is built in Python and leverages key tools and frameworks:</p>
            <ul>
              <li><code>Semantic Kernel</code>: Manages embedding, retrieval, and service coordination.</li>
              <li><code>Qdrant</code>: A local vector database used for storing and retrieving memory in offline mode.</li>
              <li><code>Azure AI Search</code>: A cloud-based vector + keyword search system for online RAG.</li>
              <li><code>Pydantic</code>: Used for the ElderlyUserMemory model to define the schema for stored memories.</li>
              <li><code>GraniteEmbeddingService</code>: A custom embedding class using IBM’s Granite model.</li>
              <li><code>FastEmbed</code>: Provides sparse and late interaction embeddings in offline mode.</li>
              <li><code>Torch & Transformers</code>: Used by the GraniteEmbeddingService to generate local embeddings.</li>
            </ul>

            <h5>Online RAG: Azure AI Search Integration</h5>
            <p><strong>Search</strong><br>
            When running in online mode, the user’s input message (retrieved via a POST request) is passed to the <code>search_memory()</code> function along with the active SemanticKernel instance. The kernel internally stores a collection service (an instance of AzureAISearchCollection) and a vectorizer, which generates embeddings using <code>AzureTextEmbedding</code>.</p>

            <pre><code class="language-python">collection = kernel.services["collection"]
            vectorizer = kernel.services["vectorizer"]</code></pre>

            <p>The search flow begins by performing text-based search:</p>

            <pre><code class="language-python">text_results = await collection.text_search(search_text=query, top_k=top_k)</code></pre>

            <p>Simultaneously, vector-based search is performed by generating a query embedding:</p>

            <pre><code class="language-python">query_vector = (await vectorizer.kernel.get_service("embedding").generate_raw_embeddings([query]))[0]
            vector_results = await collection.vectorized_search(vector=query_vector, options=...)</code></pre>

            <p>The results of both searches are then merged using a Reciprocal Rank Fusion (RRF) algorithm in <code>apply_rrf()</code> to prioritize documents that appear highly ranked in both search types. These final memory snippets are returned to the chatbot to be embedded into the prompt for contextual response generation.</p>

            <h5>Store</h5>
            <p>If the chatbot determines that the user input should be remembered (i.e., it’s not a one-off question), it passes the input and metadata (timestamp, category, user_id) to the <code>store_memory()</code> function. This function constructs an <code>ElderlyUserMemory</code> object:</p>

            <pre><code class="language-python">record = ElderlyUserMemory(
            id=f"{user_id}_{timestamp}_{uuid}",
            memory_text=memory_text,
            category=category,
            timestamp=datetime.utcnow().isoformat() + "Z"
            )
            record = await vectorizer.add_vector_to_records(record, ElderlyUserMemory)
            await collection.upsert(record)</code></pre>

            <p>The memory is embedded and upserted into the appropriate Azure AI Search index, associated with the specific user ID.</p>

            <h5>Offline RAG: Local Vector Search with Qdrant</h5>
            <p>In offline mode, the RAG system replaces Azure components with <code>Qdrant</code>, a high-performance vector database hosted locally. During FastAPI server startup, the <code>start_server()</code> method is triggered through the lifespan context manager. This ensures that Qdrant is automatically and reliably started with the application, even when bundled into an executable.</p>

            <pre><code class="language-python">@asynccontextmanager
            async def lifespan(app):
                if not _qdrant_manager.start_server():
                    print("Warning: Failed to start Qdrant server.")
                await initialize_chatbot()
                yield
                _qdrant_manager.stop_server()</code></pre>

            <p>When a message is received, it is passed to <code>search_memory_local()</code>, which interacts with local models through the Semantic Kernel’s <code>kernel.services</code> registry.</p>

            <p>The message is embedded into three different vector types for hybrid search:</p>
            <ul>
            <li>Dense (via <code>GraniteEmbeddingService</code>)</li>
            <li>Sparse (via <code>FastEmbed</code> using BM25)</li>
            <li>Late-Interaction (via <code>FastEmbed</code>)</li>
            </ul>

            <pre><code class="language-python">dense_vectors = await dense_model.generate_raw_embeddings([query])
            sparse_vectors = bm25_model.query_embed(query)
            late_vectors = late_model.query_embed(query)</code></pre>

            <p>These vectors are then sent as a hybrid query to <code>Qdrant</code>:</p>

            <pre><code class="language-python">vector_results = await qdrant_client.query_points(
            QDRANT_COLLECTION,
            prefetch=[...],  # For dense and sparse
            query=late_vectors,
            using="late_interaction_embedding",
            with_payload=True,
            limit=5,
            )</code></pre>

            <p>The retrieved results (stored in <code>result.payload["memory_text"]</code>) are passed back to the chatbot for prompt composition. This ensures the model can generate replies informed by relevant past interactions, even offline.</p>

            <p>If the message should be remembered, it’s passed to <code>store_memory_local()</code> where embeddings are computed and wrapped into a <code>PointStruct</code>. These are inserted into the <code>Qdrant</code> collection using:</p>

            <pre><code class="language-python">await qdrant_client.upsert(collection_name=QDRANT_COLLECTION, points=[record])</code></pre>

            <p>Each memory still receives a UUID and timestamp for tracking.</p>

            <h5>Unified Schema via Pydantic</h5>
            <p>Both systems share a common schema based on the <code>ElderlyUserMemory</code> <code>Pydantic</code> model. This defines how memory records are structured, embedded, and stored:</p>

            <pre><code class="language-python">@vectorstoremodel
            class ElderlyUserMemory(BaseModel):
            id: Annotated[str, VectorStoreRecordKeyField]
            memory_text: Annotated[str, VectorStoreRecordDataField(...)]
            memory_vector: Annotated[list[float], VectorStoreRecordVectorField(...)]
            category: Annotated[str, VectorStoreRecordDataField()]
            timestamp: Annotated[str, VectorStoreRecordDataField()]
            )</code></pre>

            <p>For <code>Qdrant</code>, the schema is manually matched by creating a <code>PointStruct</code> with equivalent keys and vector fields. For Azure, the Semantic Kernel internally parses and embeds the <code>Pydantic</code> model using annotations to create the appropriate index structure.</p>
        </section>

        <section id="speech" class="section">
            <h4>Speech functionalities (TTS/STT)</h4>
            <h5>Speech-to-Text Functionality (STT)</h5>
            <p>To improve accessibility and allow more natural interaction, our system supports voice input through speech-to-text conversion. This feature is available in both online and offline modes, and in both cases, it uses Azure’s Speech Services for its robustness and accuracy.</p>
          
            <p><strong>Technologies Used:</strong></p>
            <ul>
              <li><code>Azure Speech Service</code>: Converts live audio into text using cloud-based recognition.</li>
              <li><code>Microsoft Cognitive Services Speech SDK</code> (in JS): Allows the frontend to directly access Azure STT.</li>
              <li><code>React Hooks</code> (<code>useState</code>, <code>useCallback</code>, <code>useEffect</code>, <code>useRef</code>): Manage recognizer state and UI interactivity.</li>
              <li><code>FastAPI Endpoint</code> <code>/api/azure-credentials</code>: Provides secure access to Azure subscription key and region.</li>
            </ul>
          
            <h5>How It Works (Frontend Flow)</h5>
            <p>When the user clicks the microphone button, the React frontend calls the <code>startRecording</code> method from the VoiceContext:</p>
          
            <pre><code class="language-js">const { startRecording } = useVoice();</code></pre>
          
            <p>This invokes the <code>initializeAzureRecognizer()</code> function. Inside, the app:</p>
            <ol>
              <li>Fetches secure credentials from the backend (<code>/api/azure-credentials</code>).</li>
              <li>Initializes the Azure Speech Recognizer using the Speech SDK:</li>
            </ol>
          
          <pre><code class="language-js">const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(subscriptionKey, region);
          const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
          const recognizer = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);</code></pre>
          
            <li>Registers event listeners for:
              <ul>
                <li><code>.recognizing</code>: for interim transcription</li>
                <li><code>.recognized</code>: for finalized text</li>
                <li><code>.canceled</code>: for errors or interruptions</li>
                <li><code>.sessionStarted</code> / <code>.sessionStopped</code>: to control UI states</li>
              </ul>
            </li>
          
            <li>Starts continuous speech recognition:</li>
            <pre><code class="language-js">recognizer.startContinuousRecognitionAsync(...)</code></pre>
          
            <p>As the user speaks, recognized speech is passed into the state via:</p>
            <pre><code class="language-js">setRecognizedText(event.result.text);</code></pre>
            <p>The final <code>recognizedText</code> is then shown in the chat input.</p>
          
            <h5>Voice Control and Graceful Shutdown</h5>
            <p>The recognizer can be stopped manually with <code>stopRecording()</code> or automatically when voice commands like "stop listening" or "exit program" are detected. If a recording session ends, the recognizer is either closed or reused for future sessions, improving performance and avoiding memory leaks.</p>
          
            <p>Recognizer cleanup logic is also included on component unmount:</p>
            <pre><code class="language-js">useEffect(() => {
            return () => {
              stopCurrentAudio();
              recognizer?.stopContinuousRecognitionAsync();
              recognizer?.close();
            };
          }, []);</code></pre>
          
            <p>Users can speak naturally into their device’s microphone, and the system will transcribe it in real time using Azure STT. Recognized text is reflected in the chat UI and passed to the backend chatbot logic for processing. Voice commands like “stop listening” are interpreted in context and help control the session naturally. This makes the feature both technically robust and friendly for elderly users, as intended.</p>
          
            <h5>Text-to-Speech (TTS) Functionality</h5>
            <p>To make the chatbot more friendly and accessible, especially for elderly users, we implemented Text-to-Speech (TTS). This allows the chatbot to speak its responses out loud using natural-sounding voices. We support both online (cloud-based) and offline TTS, depending on the user's selected configuration.</p>
          
            <p><strong>Technologies Used</strong></p>
            <ul>
              <li><code>Azure Speech Services</code> (Online): Used for high-quality neural voice synthesis in English, with support for both male and female voices.</li>
              <li><code>Coqui TTS</code> (Offline): Lightweight, open-source TTS model that runs locally using PyTorch and torchaudio.</li>
              <li><code>FastAPI</code>: Used to expose the <code>/api/text-to-speech</code> endpoint for serving audio responses as binary WAV files.</li>
              <li><code>React</code> (Frontend): Sends POST requests to the backend and plays back the received audio using a native HTML5 <code>&lt;audio&gt;</code> element.</li>
            </ul>
          
            <h5>How It Works (End-to-End Flow)</h5>
            <p>When a chatbot response is generated, the frontend calls this endpoint:</p>
            <pre><code class="language-js">await fetch('/api/text-to-speech', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ message: responseText }),
          });</code></pre>
          
            <h6>Backend: /api/text-to-speech (FastAPI)</h6>
            <p>This triggers the following logic in the backend:</p>
          
            <pre><code class="language-python">@app.post("/api/text-to-speech")
          async def convert_text_to_speech(input_data: UserInput):
            if config.USE_OLLAMA:
              audio_bytes = speak_text_to_bytes(input_data.message, gender=gender)
            else:
              audio_bytes = text_to_speech_to_bytes(input_data.message, gender=gender)
            return Response(content=audio_bytes, media_type="audio/wav")</code></pre>
          
            <p>If the user is in online mode, the function <code>text_to_speech_to_bytes</code> is called, which uses Azure TTS. If the user is in offline mode, it instead calls <code>speak_text_to_bytes</code>, which uses Coqui TTS.</p>
          
            <h6>Online Mode (Azure TTS)</h6>
            <p>The <code>text_to_speech_to_bytes</code> function performs the following:</p>
          
            <pre><code class="language-python">speech_config = speechsdk.SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SPEECH_REGION)
          speech_config.speech_synthesis_voice_name = "en-US-GuyNeural"  # or Jenny for female
          synthesizer = speechsdk.SpeechSynthesizer(speech_config, audio_config)
          result = synthesizer.speak_text_async(text).get()</code></pre>
          
            <p>The audio is first synthesized and saved to a temporary WAV file. If synthesis is successful, the file is read back as bytes and returned to the user. The file is then safely deleted using a finally block to clean up resources. This ensures high-quality, expressive speech using cloud-powered voices.</p>
          
            <h6>Offline Mode (Coqui TTS)</h6>
            <p>The <code>speak_text_to_bytes</code> function handles the local model:</p>
          
            <pre><code class="language-python">audio_data = tts_model.tts(text=text, speaker=speaker)
          audio_tensor = torch.tensor(audio_data).unsqueeze(0)
          torchaudio.save(buffer, audio_tensor, sample_rate, format="wav")</code></pre>
          
            <ol>
              <li>Text is passed into Coqui’s pre-trained TTS model.</li>
              <li>The audio is returned as a NumPy array, converted into a PyTorch tensor, and saved as a WAV byte stream.</li>
              <li>The final output is streamed to the frontend as playable audio.</li>
            </ol>
          
            <p>Although offline voices are less expressive, they are fast and private, making them ideal for users without internet access.</p>
          
            <h5>Frontend Playback</h5>
            <p>On receiving the audio (offline/online), the React frontend creates an Audio object and plays it like so:</p>
          
            <pre><code class="language-js">const audio = new Audio(URL.createObjectURL(audioBlob));
          audio.play();</code></pre>
          
            <p>We also handle edge cases like overlapping audio, small audio sizes, or muting the output entirely if needed.</p>
          
            <p>With support for both cloud-based and offline text-to-speech, this feature provides a smooth, engaging experience for users. The flexibility in choosing between natural-sounding Azure voices or fully local Coqui synthesis makes the application suitable for both accessibility-focused use cases and resource-constrained environments.</p>
          </section>
          
        <section id="prompt-engineering" class="section">
  <h4>Prompt Engineering</h4>
  <p>Prompt engineering forms a key part of how our assistant understands and responds to user input. A well-crafted prompt ensures that the chatbot remains contextually aware, aligns with our intended tone, and produces reliable and relevant responses. Without comprehensive prompting, our goals of preserving chat history continuity, minimising hallucinations, and maintaining instructional accuracy are severely compromised.</p>

  <p>Our prompt achieves our goal with four main practices. First, the assistant's persona is clearly defined as a kind AI companion, anchoring its tone and behaviour for sensitive, human-centric interactions. This is followed by including the ten most recent chat history with <code>truncation_reducer</code> to maintain continuity, which is essential for conversations to flow more naturally with the chatbot. Finally, Retrieval-Augmented Generation (RAG) is integrated via <code>past_memory</code>, allowing the assistant to access long-term context for an enriched response. Strict instructions serve as safeguards against hallucination, topic blending, or over-association–problems common during our initial testing phase with the chatbot. By constraining the assistant to only respond to the user message and limiting output length, we make conversations clearer and easier to follow. This approach creates a stable, natural experience, just like talking with a friend!</p>

  <pre><code class="language-python">"""
&lt;&lt;INSTRUCTIONS&gt;&gt;
This is a conversation between you, a kind AI companion, and a user.
A description of who you are and how you should behave:
{{ $system_message }}

Use the chat history to maintain context and provide relevant responses:
{{ $truncation_reducer }}

Use past memory to recall relevant past conversation, only use when necessary and related to the current conversation:
{{ $past_memory }}

**STRICT INSTRUCTION:** 
- **DO NOT assume relationships between unrelated topics unless the user explicitly requests a connection.** 
- **DO NOT blend or mix topics from past memory or chat history unless directly relevant.** 
- **If the user switches topics, treat it as a new, separate conversation.** 
- **Only respond to the user message and nothing else. DO NOT create information unless from past memory or chat history.**
- **Keep responses 2-3 sentences max unless asked for more.**
&lt;&lt;END INSTUCTIONS&gt;&gt;

Current user message:
{{ $user_input }}

**Your Response:** 
"""</code></pre>

  <p>The <code>SYSTEM_MESSAGE</code> variable defines the core personality and behavior of our chatbot:</p>

  <pre><code class="language-python">SYSTEM_MESSAGE = 
"""
You are a kind and caring chatbot designed to provide companionship and support for elderly users, including those experiencing memory issues or dementia.
Your name is Companio, and you have one goal: to bring warmth, understanding, and gentle companionship to those who need it.
You are patient, empathetic, and adapt your responses to make conversations easy and enjoyable.
Speak in a simple, warm tone and always make the user feel valued and understood.
"""</code></pre>
</section>

        <!-- frontend -->
        <section id="frontend" class="section">
            <h2>Frontend Implementation</h2>
            
            <h4>Avatar Interface</h4>
            <p>The frontend interface features two predefined selectable avatars which bring Companio to life. Both avatar components utilise 3D model rendering, animation handling, and dynamic facial expressions to provide a natural experience for the user. The avatars are structured using React Three Fiber and utilise <code>useGLTF</code>, <code>useFBX</code>, and <code>useAnimations</code> from <code>@react-three/drei</code> to load and animate the assistants. Both avatars share similar logic for animation startup, voice detection and facial expression, and animation playback depending on the relevant action.</p>
            
            <p>Initially, multiple FBX animations are loaded and renamed for simplicity:</p>
            
            <pre><code class="language-js">const { animations: idleAnimation } = useFBX(idleAnimationPath);
const { animations: greetingAnimationDefault } = useFBX(greetingDefaultPath);
// ...other animations...</code></pre>
            
            <p>These are then passed to <code>useAnimations()</code> alongside a reference to the model group:</p>
            
            <pre><code class="language-js">const { actions } = useAnimations(
[
  idleAnimation[0],
  greetingAnimationDefault[0],
  greetingAnimationAlternative[0],
  talkingDefault[0],
  talkingAlternative[0]
],
group
);</code></pre>
            
            <p>The avatar also listens to hooks from <code>useVoice</code> and <code>useChat</code> to determine if the speaking animation should be played. If Companio is actively speaking, the avatar will play talking animations continuously until the hook sets the <code>isSpeaking</code> state to false. The avatar will also perform the relevant facial expressions for talking. As specific morph targets for mouth movements are unsupported in both avatars, random movements of the mouth and lips are done to simulate talking:</p>
            
            <pre><code class="language-js">if (effectivelySpeaking && lastMessage?.sender === 'ai') {
const initialAnimation = Math.random() > 0.5 ? "DefaultTalking" : "AlternativeTalking";
setAnimation(initialAnimation);

clearInterval(mouthAnimationInterval);
clearTimeout(talkingTimeout);
clearInterval(animationSwitchInterval);

animationSwitchInterval = setInterval(() => {
  setAnimation(prev =>
    prev === "DefaultTalking" ? "AlternativeTalking" : "DefaultTalking"
  );
}, ANIMATION_SWITCH_INTERVAL);

// Create an interval that changes mouth movements continuously
mouthAnimationInterval = setInterval(() => {
  const mouthOpenValue = Math.random() * 0.5 + 0.5; 
  const mouthSmileValue = mouthOpenValue * 0.3 + 0.3;</code></pre>
            
            <p>If text-to-speech is disabled, the avatar simulates speaking for a set number of time:</p>
            
            <pre><code class="language-js">if (modelConfig && modelConfig.use_speech_output === false) {
setLocalIsSpeaking(true);

const timer = setTimeout(() => {
  setLocalIsSpeaking(false);
}, FALLBACK_SPEAKING_DURATION); // 3 seconds

return () => clearTimeout(timer);
}</code></pre>
            
            <p>The facial expressions are controlled via morph target influences. For example, when speaking:</p>
            
            <pre><code class="language-js">if (nodes.Wolf3D_Head?.morphTargetInfluences) {
nodes.Wolf3D_Head.morphTargetInfluences[mouthOpenIndex] = mouthOpenValue;
nodes.Wolf3D_Head.morphTargetInfluences[mouthSmileIndex] = mouthSmileValue;
}
}, MOUTH_UPDATE_INTERVAL);</code></pre>
            
            <p>There is also a randomAnimation variable which plays a random animation from the pool of animations before reverting to idle after completion:</p>
            
            <pre><code class="language-js">const randomAnimation = availableOptions[Math.floor(Math.random() * availableOptions.length)];
setAnimation(randomAnimation);</code></pre>
            
            <p>These random animations are then passed to the Experience component to play random animations if the avatar is clicked. This is achieved by placing an invisible 3D mesh over the avatar. Upon clicking, <code>triggerRandomAnimation</code> will play out random animations.</p>
            
            <p>Avatar click area:</p>
            
            <pre><code class="language-js">const AvatarClickArea = ({ onAvatarClick, cameraZoomed }) => {
// Cylinder to represent avatar's body
const radius = 0.3;
const height = cameraZoomed ? 3 : 3.5;
const yPosition = cameraZoomed ? 1.5 : 1.75;</code></pre>
            
            <p>The avatar will also greet the user during first render:</p>
            
            <pre><code class="language-js">if (isFirstGreeting) {
setAnimation("AlternativeGreeting");
setDefaultSmile();

const greetingDuration = actions["AlternativeGreeting"]?.getClip().duration * 500 || 3000;
setTimeout(() => {
  setIsFirstGreeting(false);
  setAnimation("Idle");
}, greetingDuration);
return;
}</code></pre>
            
            <p>The female avatar generally follows the same logic for animation rendering and facial expressions. However, there is also an added feature of periodical blinking for a natural feel to the avatar. This is achieved by opening and closing the eye periodically every 2-6 seconds.</p>

            <h4>Memory Games</h4>
            <p>The frontend interface also features two self-contained mini games designed to be fun, engaging, and a great way to give your brain a workout. These games offer a lighthearted break from chatting while still helping to sharpen your memory and focus. The games are fully built with React as a web page and share similar frameworks. Both games manage game state and user interactions using <code>useState</code>, timers with <code>useRef</code>, and side effects like countdown and cleanup via <code>useEffect</code>. They also include interface modals and routing navigation, and implement visual celebrations and animations to enhance the gameplay experience.</p>

            <h5>Match Pairs</h5>
            <code>initializeGame()</code> creates a shuffled deck of paired cards based on difficulty:</p>

            <pre><code class="language-js">const cardPairs = [...cardsToUse, ...cardsToUse].map((card, index) => ({
              ...card,
              uniqueId: index,
              isFlipped: false,
              isMatched: false,
            }));

            const shuffledCards = shuffleArray([...cardPairs]);
            setCards(shuffledCards);</code></pre>

            <p><code>handleCardClick()</code> manages the matching logic by checking the flipped cards for matched pairs and handling mismatches:</p>

            <pre><code class="language-js">if (firstCard.id === secondCard.id) {
              updatedCards[firstCardIndex].isMatched = true;
              updatedCards[secondCardIndex].isMatched = true;

              setLastMatchedPair(firstCard.id);
              setTimeout(() => setLastMatchedPair(null), 1500);
              
              const newMatchedPairs = [...matchedPairs, firstCard.id];
              setMatchedPairs(newMatchedPairs);
              setFlippedCards([]);

            } else {
              setTimeout(() => {
                updatedCards[firstCardIndex].isFlipped = false;
                updatedCards[secondCardIndex].isFlipped = false;
                setCards([...updatedCards]);
                setFlippedCards([]);
              }, 1000);
            }</code></pre>

            <p>The <code>useEffect</code> handles timer tracking by starting and stopping the game on completion:</p>

            <pre><code class="language-js">useEffect(() => {
              if (gameStarted && !gameCompleted) {
                timerRef.current = setInterval(() => {
                  setTimer(prevTimer => prevTimer + 1);
                }, 1000);
                
                return () => {
                  clearInterval(timerRef.current);
                };
              }
            }, [gameStarted, gameCompleted]);</code></pre>

            <p>When all pairs are matched, the game state is set to completed and a congratulatory animation will play. The confetti is styled using CSS.</p>

            <pre><code class="language-js">if (newMatchedPairs.length === (
              difficulty === 'easy' ? 3 :
              difficulty === 'medium' ? 6 : 8
            )) {
              setGameCompleted(true);
              setShowConfetti(true);
              clearInterval(timerRef.current);
            }</code></pre>

            <h5>Simon Game</h5>
            <p><code>playPattern()</code> handles the main game logic where it appends a new random pad to the sequence during each round and replays it using timed callbacks:</p>

            <pre><code class="language-js">const playPattern = (patternToPlay) => {
              setIsPlayingPattern(true);
              setUserPattern([]);
              
              patternTimers.current.forEach(timer => clearTimeout(timer));
              patternTimers.current = [];
              
              patternToPlay.forEach((pad, index) => {
                const timer = setTimeout(() => {
                  activatePad(pad);
                  
                  if (index === patternToPlay.length - 1) {
                    const endTimer = setTimeout(() => {
                      setIsPlayingPattern(false);
                    }, 700);
                    patternTimers.current.push(endTimer);
                  }
                }, index * 700);
                patternTimers.current.push(timer);
              });
            };</code></pre>

            <p><code>handlePadClick()</code> checks if the user's input follows the pattern set by <code>playPattern</code>:</p>

            <pre><code class="language-js">if (pattern[currentIndex] !== newUserPattern[currentIndex]) {
              handleGameOver();
              return;
            }

            if (newUserPattern.length === pattern.length) {
              // User completed the pattern
              handleSuccess();
            }</code></pre>

            <p>A <code>useEffect()</code> tracks high scores by saving and retrieving scores via localStorage:</p>

            <pre><code class="language-js">useEffect(() => {
              const savedScore = localStorage.getItem('simonHighScore');
              if (savedScore) {
                setHighScore(parseInt(savedScore, 10));
              }
            }, []);</code></pre>

            <p>The game also utilises the same logic for confetti celebration and plays the animation after every 5 rounds.</p>
        </section>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="js/scripts.js"></script>
</body>

</html>